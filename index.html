<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="keywords" content="eric11eca, Zeming Chen, iit roorkee, gatech, georgia tech, nocaps, ahmedabad, atlanta, georgia">

  <!-- fall back to site title if page has no title -->
  <title>
    
       Zeming Chen
   
  </title>
  <meta itemprop="description" name="description" content="Visiting research scholar at Georgia Tech, IIT Roorkee Alum." />
  <link rel="canonical" href="https://eric11eca.github.io/">

  <!-- Favicon -->
  <link rel="shortcut icon" href="/static/ico/favicon.ico" type="image/x-icon" />
  <link rel="apple-touch-icon" href="/static/ico/apple-touch-icon.png" />
  <link rel="apple-touch-icon" sizes="72x72" href="/static/ico/apple-touch-icon-72x72.png" />
  <link rel="apple-touch-icon" sizes="120x120" href="/static/ico/apple-touch-icon-120x120.png" />
  <link rel="apple-touch-icon" sizes="144x144" href="/static/ico/apple-touch-icon-144x144.png" />
  <link rel="apple-touch-icon" sizes="152x152" href="/static/ico/apple-touch-icon-152x152.png" />

  <!--Open Graph Related Stuff-->
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Zeming Chen">
  <meta property="og:url" content="https://eric11eca.github.io/">
  <meta property="og:site_name" content="Zeming Chen">
  <meta property="og:image" content="https://eric11eca.github.io/static/img/kd.jpg"/>
  <meta property="og:image:url" content="https://eric11eca.github.io/static/img/kd.jpg"/>
  <meta property="og:description" content="Visiting research scholar at Georgia Tech, IIT Roorkee Alum.">

  <!--Twitter Card Stuff-->
  <meta name="twitter:card" content="summary_large_image"/>
  <meta name="twitter:title" content="Zeming Chen">
  <meta name="twitter:description" content="Visiting research scholar at Georgia Tech, IIT Roorkee Alum.">
  <meta name="twitter:creator" content="@eric11eca">

  <meta name="twitter:image" content="https://eric11eca.github.io/static/img/kd.jpg"/>
  <meta name="twitter:url" content="https://eric11eca.github.io/static/img/kd.jpg"/>
  <meta name="twitter:description" content="Visiting research scholar at Georgia Tech, IIT Roorkee Alum."/>
  <meta name="twitter:domain" value="eric11eca.github.io">

  <meta name="twitter:label1" value="Visiting Research Scholar"/>
  <meta name="twitter:data1" value="Georgia Tech"/>

  <!-- CSS -->
  <link rel="stylesheet" href="static/css/bootstrap-grid.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
  <link rel="stylesheet" href="static/css/main.css">
  <link rel="stylesheet" href="static/css/main_second.css">
  <link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">

  <!-- Search Engine Optimization -->
  <meta name="google-site-verification" content="googleca51599afffe0f7e.html" />
</head>


<body>
  <nav id="navbar">
  <div class="container">
    <div class="row">
      <div class="col-md-12 col-sm-12 col-xs-12 col-12">
        <div id="site-title-container">
          <a id="site-title" href="">Zeming Chen</a>
        </div>
        <div id="navbar-ul-container">
          <ul id="navbar-ul">
            <li class="nav-listitem">
              <a href="#publications">Publications</a>
            </li>
            <li class="nav-listitem">
              <a target="_blank" href="./contact/Zeming_Chen_Resume.pdf">CV</a>
            </li>
            <li class="nav-listitem">
              <a target="_blank" href="https://scholar.google.com/citations?user=-gqyv8cAAAAJ&hl=en">Google Scholar</a>
            </li>
          </ul>
        </div>
      </div>
    </div>
    <hr id="navbar-border">
  </div>
</nav>


  <div class="container">
    <div class="content">
      <div class="container">
  <div class="row">
    <!-- Display picture and social media links -->
    <div class="col-md-4 col-sm-4 col-xs-12">
      <div class="row">
        <!-- Display picture -->
        <div class="col-md-12 col-sm-12 col-xs-12" style="padding-left: 0">
          <img alt="Zeming Chen" title="Zeming Chen" id="display-picture" src="contact/profile.png" />
        </div>
        <!-- Social media links -->
        <div class="col-md-12 col-sm-12 col-xs-12" style="padding-left: 0; text-align: center">
          <ul id="social-media-ul">
            <li class="social-media">
              <a target="_blank" href="https://github.com/eric11eca" itemprop="sameAs">
                <i class="fab fa-fw fa-github" aria-hidden="true"></i>
                <span>eric11eca</span>
              </a>
            </li>
            <li class="social-media">
              <a target="_blank" href="https://www.linkedin.com/in/zeming-chen-769985123/" itemprop="sameAs">
                <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i>
                <span>eric11eca</span>
              </a>
            </li>
            <!--li class="social-media">
              <a target="_blank" href="https://instagram.com/eric11eca" itemprop="sameAs">
                <i class="fab fa-fw fa-instagram" aria-hidden="true"></i>
                <span>eric11eca</span>
              </a>
            </li-->
          </ul>
        </div>
      </div>
    </div>
    <!-- About -->
    <div class="col-md-8 col-sm-8 col-xs-12">
      <p>
        I am a senior undergraduate student studying computer science 
        and mathematics at Rose-Hulman Institute of Technology. 
        I plan to attend graduate school after graduation and pursue a Ph.D. degree 
        in the field of natural language understanding and reasoning. 
      </p>
      <p>
        My research interests are natural language processing and understanding, 
        particularly in building models capable of human-level inference and understanding of language. 
        My research primarily focuses on natural logic & monotonicity, 
        neural language models, computational semantics, and natural language inference. 
        I'm currently working on probing implicit knowledge from language models for 
        building better semantic representations from language, and on developing a novel 
        learning framework that train models to learn human-level reasoning process
         through meta reinforcement learning.
        <br/><br/>
      </p>
      <p style="margin-top: 20px">
        Feel free to say hi: <b>chenz16@rose-hulman.edu</b>
      </p>
    </div>
  </div>
</div>

<hr />

<h2>What's New</h2>

<p>
  <strong>[Jun 2021]</strong> Paper out on arxiv: <a href="https://arxiv.org/abs/2105.14167">
    NeuralLog: Natural Language Inference with Joint Neural and Logical Reasoning</a>.<br />
  <strong>[Jun 2021]</strong> Serving as a reviewer for AAAI Magzine.<br />
  <strong>[Jun 2021]</strong> Invited talk to IU Polarity Annotation Group and AI2 Aristo Team, on NeuralLog.<br />
  <strong>[Jun 2021]</strong> NeuralLog paper accepted by <a herf="https://sites.google.com/view/starsem2021/home">*SEM2021</a>.<br />
  <strong>[May 2021]</strong> Paper out on arxiv: <a href="https://arxiv.org/abs/2104.08659">
    Monotonicity Marking from Universal Dependency Trees</a>.<br />
  <strong>[Apr 2021]</strong> Monotonicity Marking from Universal Dependency Trees accepted by
    <a herf="https://iwcs2021.github.io/index.html">IWCS2021</a>.<br />
  <strong>[Jul 2020]</strong> Presented Attentive-tree-structures neural network @ 
    <a herf="https://typo.uni-konstanz.de/naloma20/index.html">NALOMA2020</a>.<br />
  <strong>[May 2020]</strong> Attentive-tree-structures neural network paper accepted by
    <a herf="https://typo.uni-konstanz.de/naloma20/index.html">NALOMA2020</a>.<br />
</p>
<hr />

<!-----------------------------------------------------------------------
Publications
------------------------------------------------------------------------ -->
<h2 id="publications">Publications</h2>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr>
  <td width="100%" valign="middle">
  </td>
  </tr>
  </tbody></table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

  <!--------------------------------------------------------------------------->
  <!------------------------------------------------------------------------ -->
  <!-- NeuralLog -->
  <tbody>
  <td width="30%">

    <img style="width: 80%;"src='./publications/img/NeuralLog.png'>
  </td>

  <td valign="top" width="70%">

  <p><papertitle>
  <b>NeuralLog: Natural Language Inference with Joint Neural and Logical Reasoning</b>
  </papertitle>
  <br>
  <i><u>Zeming Chen</u></i>, Qiyue Gao, <a href="https://iulg.sitehost.iu.edu/moss/">Lawrence S. Moss</a>
  <br>
  The 10th Joint Conference on Lexical and Computational Semantics (<a herf="https://sites.google.com/view/starsem2021/home">*SEM</a>), 2021
  <br>

  <!-- Additional Links -->
  <a href="https://arxiv.org/abs/2105.14167"><span class="label label-primary">paper</span></a>
  |
  <a href="https://github.com/eric11eca/NeuralLog"><span class="label label-code">code</span></a>
  <!--a href="./references/attn_tree_chen2020.txt"><span class="label label-warning">bib</span></a-->
  |
  <span class="label label-long">long</span>
  |
  <span class="label label-conference">conference</span>
  </p>

  <p>
    Both Deep learning based language models and symbolic logical inferences approaches 
    have their advantages and weaknesses on Natural Language Inference (NLI), 
    but no method has combined them togather for joint reasoning.
    We propose NeuralLog, a novel framework that conducts inference through joint logical and neural reasonings
    by modeling the NLI task as a path searching problem.
    Experiments show that NeuralLog achieves state-of-art performance on SICK and MED datasets.
  </p>
  </td> </tr> </tbody>

  <!-- Udep2Mono -->
  <tbody>
  <td width="30%">

  <!-- Image -->
  <img style="width: 80%;"src='./publications/img/udep2mono.png'>
  </td>

  <!-- Paper Info -->
  <td valign="top" width="70%">

  <p><papertitle>
  <b>Monotonicity Marking from Universal Dependency Trees</b>
  </papertitle>
  <br>
  <i><u>Zeming Chen</u></i>, Qiyue Gao
  <br>
  The 14th International Conference on Computational Semantics (<a herf="https://iwcs2021.github.io/index.html">IWCS</a>), 2021
  <br>

  <!-- Additional Links -->
  <a href="https://arxiv.org/abs/2104.08659"><span class="label label-primary">paper</span></a>
  |
  <a href="https://github.com/eric11eca/Udep2Mono"><span class="label label-code">code</span></a>
  <!--a href="./references/attn_tree_chen2020.txt"><span class="label label-warning">bib</span></a-->
  |
  <span class="label label-long">long</span>
  |
  <span class="label label-conference">conference</span>
  </p>

  <p>
    Dependency parsing is a tool widely used in the field of Natural language processing.
    However, there is hardly any work that connects dependency parsing to monotonicity,
    an essential part of logic and linguistic semantics.
    We propose a system that automatically annotates
    monotonicity information based on Universal Dependency parse trees.
    Results show that our system outperforms NatLog and ccg2mono on a small evaluation dataset.
  </p>
  </td> </tr> </tbody>

  <!-- Attentive Tree-structured Network -->
  <tbody>
    <td width="30%">

    <img style="width: 50%; margin-left: 15%;" src='./publications/img/att_tree_lstm.png'>
    </td>

    <td valign="top" width="70%">

    <p><papertitle>
    <b>Attentive Tree-structured Network for Monotonicity Reasoning</b>
    </papertitle>
    <br>
    <i><u>Zeming Chen</u></i>
    <br>
    The 1st Workshop on Machine Learning Meets Natural Logic (<a herf="https://typo.uni-konstanz.de/naloma20/index.html">NLOMA</a>), 2020
    <br>

    <!-- Additional Links -->
    <a href="https://arxiv.org/abs/2101.00540"><span class="label label-primary">paper</span></a>
    |
    <a href="./references/attn_tree_chen2020.txt"><span class="label label-warning">bib</span></a>
    |
    <span class="label label-long">long</span>
    |
    <span class="label label-other">workshop</span>
    </p>

    <p>
      Many state-of-art neural models designed for monotonicity reasoning perform poorly on downward inference. 
      To address this shortcoming, we developed an attentive tree-structured neural network. 
      It consists of a tree-based long-short-term-memory network (Tree-LSTM) with soft attention. 
      It is designed to model the syntactic parse tree information from the sentence pair of a reasoning task. 
      A self-attentive aggregator is used for aligning the representations of the premise and the hypothesis. 
      We present our model and evaluate it using the Monotonicity Entailment Dataset (MED). 
      We show and attempt to explain that our model outperforms existing models on MED.
    </p>
    </td> </tr> </tbody>

</table>
<hr />

<!-----------------------------------------------------------------------
Research
------------------------------------------------------------------------ -->
<h2>Research</h2>

<div class="paper-container row">
  <div class="paper-title col-md-12 col-sm-12 col-xs-12">
    Neural-symbolic Inference
  </div>
  <div class="paper-authors col-md-12 col-sm-12 col-xs-12">
    Currently, models for Natural Language Inference (NLI) are too one-dimensional.
    They are either deep-learning-based models or logic-based models.
    Deep-learning-based neural language models can achieve SOTA performance on multiple benchmarks
    and even reach the so-called "superhuman performance."
    They are good at handling inferences involving syntactic variation,
    commonsense reasoning, social reasoning, and background knowledge.
    On the other hand, symbolic-logic-based systems show good performance on small challenge datasets containing complex linguistic phenomena
    through logic or semantic formalism.
    However, both sides show some weaknesses.
    Neural models lack generalization ability, adopt fallible heuristics,
    exploit dataset artifacts, and fail to do well on complex linguistic inferences.
    Logic models show lower robustness and lack world and commonsense knowledge.
    My research focuses on combining these two types of models into joint reasoning: neural-symbolic inference.
    My principle is that neural networks and logic formalisms are both excellent inference tools,
    but they require guidance and controlling to assign them into appropriate problem domains.
    My work uses various neural and symbolic inference models, including neural language models, natural logic, and semantics.
    I also explore multiple controller methods such as Neural Architecture Search,
    Deep Reinforcement and Meta-Learning, and evolutionary strategies.
  </div>
  <!-- arxiv and website link >
  <div class="paper-links col-md-12 col-sm-12 col-xs-12">
    <a class="paper-link button-div" href="//arxiv.org/abs/2012.04630" target="_blank">
      arxiv/2012.04630
    </a>
    <a class="paper-link button-div" href="//blog.einstein.ai/casting-your-model-learning-to-localize-improves-self-supervised-representations/" target="_blank">
      blog
    </a>
    <a class="paper-link button-div" href="/static/bibliography/cast_bibtex.txt" target="_blank">
      bibtex
    </a>
  </div>
  <div class="paper-banner col-md-12 col-sm-12 col-xs-12">
    <img src="/static/img/research/cast_banner.jpg" alt="cast model" title="CASTing Your Model: Learning to Localize Improves Self-Supervised Representations" />
  </div-->
</div>
<hr />

<h2>Open Source Projects</h2>

<div class="projects-container row">
  <!-- project: AutomatonStudio -->
  <div class="col-md-12 col-sm-12 col-xs-12">
    <div class="project-container row">
      <div class="project-title col-md-12 col-sm-12 col-xs-12">
        AutomatonStudio <div class="project-stars button-div" id="visdial-challenge-starter-pytorch-stars"></div>
      </div>
      <div class="project-description col-md-12 col-sm-12 col-xs-12">
        Starter code for the Visual Dialog Challenge. 
        Built using PyTorch v1.0 and provides out of the box support with CUDA 9 and CuDNN 7. 
        Provides a simple implementation of Late Fusion encoder and Discriminative decoder. 
        Complete with efficient scripts for data preprocessing, image feature extraction, 
        training and evaluation, along with support to generate a submission file for the challenge.
      </div>
    </div>
  </div>
</div>

<div class="projects-container row">
  <!-- project: lang-emerge-parlai -->
  <div class="col-md-6 col-sm-12 col-xs-12">
    <div class="project-container row">
      <div class="project-title col-md-12 col-sm-12 col-xs-12">
        lang-emerge-parlai <div class="project-stars button-div" id="lang-emerge-parlai-stars"></div>
      </div>
      <div class="project-description col-md-12 col-sm-12 col-xs-12">
        Implementation of the paper "Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog" by <a href="//arxiv.org/abs/1706.08502" target="_blank">Kottur et al (EMNLP 2017)</a>, using PyTorch and ParlAI.
      </div>
      <div class="project-banner col-md-12 col-sm-12 col-xs-12">
        <img src="/static/img/projects/lang_emerge_banner.jpg" alt="lang-emerge-parlai" title="language emergence" />
      </div>
    </div>
  </div>

  <!-- project: ntm-pytorch -->
  <div class="col-md-6 col-sm-12 col-xs-12">
    <div class="project-container row">
      <div class="project-title col-md-12 col-sm-12 col-xs-12">
        ntm-pytorch <div class="project-stars button-div" id="ntm-pytorch-stars"></div>
      </div>
      <div class="project-description col-md-12 col-sm-12 col-xs-12">
        Implementation of Neural Turing Machines introduced in <a href="//arxiv.org/abs/1410.5401" target="_blank">Graves et al (2014)</a>, using PyTorch. Supports training and evaluation on four out of six tasks described in the paper.
      </div>
      <div class="project-banner col-md-12 col-sm-12 col-xs-12">
        <img src="/static/img/projects/ntm_banner.jpg" alt="ntm-pytorch" title="neural turing machines" />
      </div>
    </div>
  </div>
</div>

<div class="projects-container row">
  <!-- project: trianglify -->
  <div class="col-md-6 col-sm-12 col-xs-12">
    <div class="project-container row">
      <div class="project-title col-md-12 col-sm-12 col-xs-12">
        trianglify <div class="project-stars button-div" id="trianglify-stars"></div>
      </div>
      <div class="project-description col-md-12 col-sm-12 col-xs-12">
        Trianglify is a highly customizable library to generate beautiful triangle art views for android. Uses the Delaunay Triangulation algorithm behind the curtains.
      </div>
      <div class="project-banner col-md-12 col-sm-12 col-xs-12">
        <img src="/static/img/projects/trianglify_banner.jpg" alt="trianglify" title="trianglify" />
      </div>
    </div>
  </div>

  <!-- project: yolog -->
  <div class="col-md-6 col-sm-12 col-xs-12">
    <div class="project-container row">
      <div class="project-title col-md-12 col-sm-12 col-xs-12">
        yolog <div class="project-stars button-div" id="yolog-stars"></div>
      </div>
      <div class="project-description col-md-12 col-sm-12 col-xs-12">
        Yolog wraps over vanilla git log and displays commit history complete with graph, timestamp, author and refs. Colors are configurable, and all standard git log commands work.
      </div>
      <div class="project-banner col-md-12 col-sm-12 col-xs-12">
        <img src="/static/img/projects/yolog_banner.jpg" alt="yolog" title="yolog" />
      </div>
    </div>
  </div>
</div>

<hr />

<h2>Talks</h2>

<script>
  // ------------------------------------------------------------------
  // Show github stars beside projects
  // ------------------------------------------------------------------
  window.githubStars = function (user, repo) {
    var xhr = new XMLHttpRequest();
    var url = "https://api.github.com/repos/" + user + "/" + repo;
    xhr.onreadystatechange = function() {
      if (xhr.readyState == 4 && xhr.status == 200) {
        document.getElementById(repo + "-stars").innerHTML = `
          <a target="_blank" href="https://github.com/` + user + `/` + repo + `" itemprop="sameAs">
            <span>` + parseInt(JSON.parse(xhr.responseText).stargazers_count) + `</span>
            <i class="fas fa-star" aria-hidden="true"></i>
          </a>`;
      }
    }
    xhr.open("GET", url);
    xhr.setRequestHeader("Accept", "application/vnd.github.v3+json");
    xhr.send();
  }

  githubStars("batra-mlp-lab", "visdial-challenge-starter-pytorch");
  githubStars("eric11eca", "lang-emerge-parlai");
  githubStars("vlgiitr", "ntm-pytorch");
  githubStars("mdg-iitr", "trianglify");
  githubStars("eric11eca", "yolog");
  githubStars("eric11eca", "digit-classifier");
  githubStars("eric11eca", "snake");
  // ------------------------------------------------------------------
</script>


    </div>
  </div>

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-120523111-1', 'auto');
  ga('send', 'pageview');
</script>

</body>

</html>

[
    {
        "title": "MEDITRON: Open Medical Foundation Models Adapted for Clinical Practice",
        "imageSrc": "images/meditron-nature.png",
        "imageSrcset": "images/meditron-nature.png 500w, images/meditron-nature.png 800w, images/meditron-nature.png 1080w, images/meditron-nature.png 1600w",
        "pdfLink": "https://www.researchsquare.com/article/rs-4139743/v1",
        "codeLink": "https://github.com/epfLLM/meditron",
        "conferenceLink": null,
        "conferenceLabel": null,
        "authors": "Zeming Chen, Angelika Romanou, Antoine Bonnet, Alejandro Hernández-Cano, Badr Alkhamissi, Kyle Matoba, et al., Physician Evaluation Group, Noémie Boillat-Blanco, Kristina Keitel, Javier Elkin, Blaise Robert, Syrielle Montariol, Mary-Anne Hartley, Martin Jaggi, Antoine Bosselut",
        "publicationInfo": "Research Square Preprint",
        "presentationType": null,
        "mediaLink": "https://ai.meta.com/blog/llama-2-3-meditron-yale-medicine-epfl-open-source-llm/",
        "mediaInfo": "Spotlight Research Blog by Meta AI",
        "summary": "Meditron, a suite of open-source large multimodal foundation models tailored to the medical field and designed to assist with clinical decision-making and diagnosis, was built on Meta Llama 2 and trained on carefully curated, high-quality medical data sources with continual input from clinicians and experts in humanitarian response."
    },
    {
        "title": "INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge",
        "imageSrc": "images/include.png",
        "imageSrcset": "images/include.png 500w, images/include.png 800w, images/include.png 1080w, images/include.png 1600w",
        "pdfLink": "https://arxiv.org/abs/2411.19799",
        "conferenceLink": null,
        "conferenceLabel": null,
        "authors": "Angelika Romanou*, Negar Foroutan, Anna Sotnikova, Zeming Chen, Sree Harsha Nelaturu, Shivalika Singh, Rishabh Maheshwary, Micol Altomare, Mohamed A. Haggag, Imanol Schlag, Marzieh Fadaee, Sara Hooker, Antoine Bosselut",
        "publicationInfo": "Arxiv Preprint",
        "presentationType": null,
        "mediaLink": ["https://cohere.com/research/papers/include-evaluating-multilingual-language-understanding-with-regional-knowledge-2024-11-29"],
        "mediaInfo": ["Spotlight Research Blog by Cohere for AI"],
        "summary": "The development of functional LLMs in many languages is bottlenecked by the lack of high-quality evaluation resources in non-English languages that focus on the regional and cultural knowledge of the environments. In this work, we construct an evaluation suite of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in a variety of regional contexts. Our novel resource, INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed."
    },
    {
        "title": "MEDITRON-70B: Scaling Medical Pretraining for Large Language Models",
        "imageSrc": "images/meditron.jpg",
        "imageSrcset": "images/meditron.jpg 500w, images/meditron.jpg 800w, images/meditron.jpg 1080w, images/meditron.jpg 1600w",
        "pdfLink": "https://arxiv.org/abs/2311.16079",
        "codeLink": "https://github.com/epfLLM/meditron",
        "conferenceLink": null,
        "conferenceLabel": null,
        "authors": "Zeming Chen, Alejandro Hernández-Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, Alexandre Sallinen, Alireza Sakhaeirad, Vinitra Swamy, Igor Krawczuk, Deniz Bayazit, Axel Marmet, Syrielle Montariol, Mary-Anne Hartley, Martin Jaggi, Antoine Bosselut",
        "publicationInfo": "Arxiv Preprint",
        "presentationType": null,
        "mediaLink": ["https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_Chapter5.pdf","https://github.com/dair-ai/ML-Papers-of-the-Week/blob/main/#top-ml-papers-of-the-week-november-27---december-3"],
        "mediaInfo": ["2024 AI Index Report Highlited Research for Science and Medicine", "Top ML Papers of the Week (by dair.ai)"],
        "summary": "Meditron is a suite of open-source medical Large Language Models (70B & 7B) adapted to the medical domain from Llama-2 through continued pretraining on a comprehensively curated medical corpus, including selected PubMed articles, abstracts, and internationally-recognized medical guidelines. Meditron-70B, finetuned on relevant training data, demonstrates high-level medical reasoning and improved domain-specific benchmark performance over Llama-2-70B, GPT-3.5, and Flan-PaLM. We democratize an optimized workflow to scale up domain-specific pretraining for medical LLMs to help revolutionize access to medical knowledge and evidence through open-source LLMs."
    },
    {
        "title": "Could ChatGPT get an engineering degree? Evaluating higher education vulnerability to AI assistants",
        "imageSrc": "images/nlp4edu.jpg",
        "imageSrcset": "images/nlp4edu.jpg 500w, images/nlp4edu.jpg 800w, images/nlp4edu.jpg 1080w, images/nlp4edu.jpg 1600w",
        "pdfLink": "https://www.pnas.org/doi/10.1073/pnas.2414955121",
        "codeLink": null,
        "journalLink": "https://www.pnas.org/",
        "journalLabel": "PNAS",
        "authors": "Beatriz Borges*, Negar Foroutan, Deniz Bayazit, Anna Sotnikova, Syrian Montariol, Tanya Nazaretzky, Mohammadreza Banaei, Alireza Sakhaeirad, Philippe Servant, Seyed Parsa Neshaei, Jibril Frej, Angelika Romanou, Gail Weiss, Sepideh Mamooler, Zeming Chen, Simin Fan, Silin Gao, Mete Ismayilzada, Debjit Paul, Philippe Schwaller, Sacha Friedli, Patrick Jermann, Tanja Käser, Antoine Bosselut, EPFL Grader Consortium, and EPFL Data Consortium",
        "publicationInfo": "Proceedings of the National Academy of Sciences of the United States of America",
        "presentationType": null,
        "summary": "Universities primarily evaluate student learning through various course assessments. Our study demonstrates that AI assistants, such as ChatGPT, can answer at least 65.8% of examination questions correctly across 50 various courses in the technical and natural sciences. Our analysis demonstrates that these capabilities render many degree programs (and their teaching objectives) vulnerable to potential misuse of these systems. These findings call for attention to assessment design to mitigate the possibility that AI assistants could entertain students from acquiring the knowledge and critical thinking skills that university programs are meant to instill."
    },
    {
        "title": "Discovering Knowledge-Critical Subnetworks in Pretrained Language Models",
        "imageSrc": "images/subnetwork.png",
        "imageSrcset": "images/subnetwork.png 500w, images/subnetwork.png 800w, images/subnetwork.png 1080w, images/subnetwork.png 1600w",
        "pdfLink": "https://arxiv.org/abs/2310.03084",
        "codeLink": null,
        "conferenceLink": "https://2024.emnlp.org/",
        "conferenceLabel": "EMNLP 2024",
        "authors": "Deniz Bayazit*, Negar Foroutan, Zeming Chen, Gail Weiss, Antoine Bosselut",
        "publicationInfo": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP 2024), To Appear",
        "presentationType": null,
        "summary": "This work investigates whether pretrained language models contain various knowledge-critical subnetworks: particular sparse computational subgraphs responsible for encoding specific knowledge the model has memorized, and proposes a multi-objective differentiable weight masking scheme to discover them."
    },
    {
        "title": "Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs",
        "imageSrc": "images/com2.png",
        "imageSrcset": "images/com2.png 500w, images/com2.png 800w, images/com2.png 1080w, images/com2.png 1600w",
        "pdfLink": "https://arxiv.org/abs/2403.07398",
        "codeLink": null,
        "conferenceLink": "https://2024.aclweb.org",
        "conferenceLabel": "ACL 2024",
        "authors": "Tianqing Fang*, Zeming Chen, Yangqiu Song, Antoine Bosselut",
        "publicationInfo": "Proceedings of The 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024)",
        "presentationType": null,
        "summary": "We present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop logical queries from CSKG and verbalizing them into multiple-choice and text generation questions. language models trained on COM2 exhibit significant improvements in complex reasoning ability, resulting in enhanced zero-shot performance in question answering and generative commonsense reasoning tasks, without expensive human annotations."
      },
    {
      "title": "RECKONING: Reasoning through Dynamic Knowledge Encoding",
      "imageSrc": "images/meta-kg.png",
      "imageSrcset": "images/meta-kg.png 500w, images/meta-kg.png 800w, images/meta-kg.png 1080w, images/meta-kg.png 1600w",
      "pdfLink": "https://arxiv.org/abs/2305.06349",
      "codeLink": "https://github.com/eric11eca/reckoning-metakg",
      "conferenceLink": "https://nips.cc/",
      "conferenceLabel": "NeurIPS 2023",
      "authors": "Zeming Chen, Gail Weiss, Eric Mitchell, Asli Celikyilmaz, Antoine Bosselut",
      "publicationInfo": "Proceedings of The Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)",
      "presentationType": null,
      "summary": "RECKONING is a bi-level learning algorithm that improves language models' reasoning ability by folding contextual knowledge into parametric knowledge through back-propagation. Compared to a fine-tuned in-context reasoning baseline initiated from the same pretrained model, we find that RECKONING shows better performance on multi-hop reasoning tasks, is more robust to distractors, and generalizes better to longer reasoning chains."
    },
    {
      "title": "DISCO: Distilling Counterfactuals with Large Language Models",
      "imageSrc": "images/distillation.png",
      "imageSrcset": "images/distillation-p-500.png 500w, images/distillation-p-800.png 800w, images/distillation-p-1080.png 1080w, images/distillation-p-1600.png 1600w",
      "pdfLink": "https://arxiv.org/abs/2212.10534",
      "codeLink": "https://github.com/eric11eca/disco",
      "conferenceLink": "https://2023.aclweb.org",
      "conferenceLabel": "ACL 2023",
      "authors": "Zeming Chen, Qiyue Gao, Antoine Bosselut, Ashish Sabharwal, Kyle Richardson",
      "publicationInfo": "Proceedings of The 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023)",
      "presentationType": null,
      "summary": "DISCO is a framework for generating counterfactual data at scale, using a large language model to generate phrasal perturbations and a task-specific teacher model to distill the data into high-quality counterfactuals. Training on DISCO's data leads to a student model that is more robust and generalizes better across distributions, and is also more sensitive in differentiating original and counterfactual examples."
    },
    {
        "title": "Mitigating Label Biases for In-context Learning",
        "imageSrc": "images/bias_sources.png",
        "imageSrcset": "images/bias_sources.png 500w, images/bias_sources.png 800w, images/bias_sources.png 1080w, images/bias_sources.png 1600w",
        "pdfLink": "https://arxiv.org/abs/2305.19148",
        "conferenceLink": "https://2023.aclweb.org",
        "conferenceLabel": "ACL 2023",
        "authors": "Yu Fei*, Yifan Hou, Zeming Chen, Antoine Bosselut",
        "publicationInfo": "Proceedings of The 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023)",
        "presentationType": null,
        "summary": "In-context learning (ICL), as a new paradigm for natural language processing (NLP), allows large language models (LLMs) to make predictions based on a few examples. However, ICL is susceptible to bias, which the choice and order of the in-context examples can cause. We propose a simple bias calibration method that significantly improves the ICL performance of GPT-J and GPT-3 on a wide range of tasks."
    },
    {
        "title": "Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding",
        "imageSrc": "images/EvaluationPip.png",
        "imageSrcset": "images/EvaluationPip-p-500.png 500w, images/EvaluationPip-p-800.png 800w, images/EvaluationPip-p-1080.png 1080w, images/EvaluationPip-p-1600.png 1600w",
        "pdfLink": "https://aclanthology.org/2022.naacl-main.234/",
        "codeLink": "https://github.com/eric11eca/curriculum-ling",
        "videoLink": "https://aclanthology.org/2022.naacl-main.234.mp4",
        "conferenceLink": "https://2022.naacl.org",
        "conferenceLabel": "NAACL 2022",
        "authors": "Zeming Chen, Qiyue Gao",
        "publicationInfo": "Proceedings of the 2022 Annual Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL-HLT 2022)",
        "presentationType": "Oral Presentation",
        "summary": "Curriculum is introduced as a new format of NLI benchmark for evaluation of broad-coverage linguistic phenomena and it is shown that this linguistic-phenomena-driven benchmark can serve as an effective tool for diagnosing model behavior and verifying model learning quality."
    },
    {
        "title": "Probing Linguistic Information For Logical Inference In Pre-trained Language Models",
        "imageSrc": "images/inferencekg_probe.png",
        "imageSrcset": "images/inferencekg_probe-p-500.png 500w, images/inferencekg_probe-p-800.png 800w, images/inferencekg_probe-p-1080.png 1080w, images/inferencekg_probe-p-1600.png 1600w, images/inferencekg_probe-p-2000.png 2000w, images/inferencekg_probe-p-2600.png 2600w, images/inferencekg_probe-p-3200.png 3200w",
        "pdfLink": "https://arxiv.org/abs/2112.01753",
        "codeLink": "https://github.com/eric11eca/inference-information-probing",
        "conferenceLink": "https://aaai.org/Conferences/AAAI-22/",
        "conferenceLabel": "AAAI 2022",
        "authors": "Zeming Chen, Qiyue Gao",
        "publicationInfo": "Proceedings of the 36th AAAI Conference on Artificial Intelligence (AAAI 2022)",
        "presentationType": "Oral Presentation",
        "summary": "This work proposes a methodology for probing knowledge for inference that logical systems require but often lack in pre-trained language model representations, and demonstrates language models' potential as semantic and background knowledge bases for supporting symbolic inference methods."
    },
    {
        "title": "NeuralLog: Natural Language Inference with Joint Neural and Logical Reasoning",
        "imageSrc": "images/NeuralLog.png",
        "imageSrcset": "images/NeuralLog-p-500.png 500w, images/NeuralLog-p-800.png 800w, images/NeuralLog-p-1080.png 1080w, images/NeuralLog-p-1600.png 1600w",
        "pdfLink": "https://aclanthology.org/2021.starsem-1.7/",
        "codeLink": "https://github.com/eric11eca/NeuralLog",
        "conferenceLink": "https://sites.google.com/view/starsem2021/home",
        "conferenceLabel": "*SEM 2021",
        "authors": "Zeming Chen, Qiyue Gao, Lawrence S. Moss",
        "publicationInfo": "Proceedings of the 10th Joint Conference on Lexical and Computational Semantics (*SEM 2021)",
        "presentationType": null,
        "summary": "This work proposes an inference framework called NeuralLog, which utilizes both a monotonicity-based logical inference engine and a neural network language model for phrase alignment, and shows that the joint logic and neural inference system improves accuracy on the NLI task and can achieve state-of-art accuracy onThe SICK and MED datasets."
    },
    {
        "title": "Monotonicity Marking from Universal Dependency Trees",
        "imageSrc": "images/udep2mono.png",
        "imageSrcset": "images/udep2mono-p-500.png 500w, images/udep2mono.png 536w",
        "pdfLink": "https://aclanthology.org/2021.iwcs-1.12/",
        "codeLink": "https://github.com/eric11eca/Udep2Mono",
        "conferenceLink": "https://iwcs2021.github.io/index.html",
        "conferenceLabel": "IWCS 2021",
        "awardLink": "https://iwcs2021.github.io/proceedings.html",
        "awardLabel": "Award",
        "authors": "Zeming Chen, Qiyue Gao",
        "publicationInfo": "Proceedings of the 14th International Conference on Computational Semantics (IWCS 2021)",
        "presentationType": "🏆 Outstanding Paper Award 🏆",
        "summary": "This paper presents a system that automatically annotates monotonicity information based on Universal Dependency parse trees, which utilizes surface-level monotonicism facts about quantifiers, lexical items, and token-level polarity information. Evaluations on shwo that the proposed system achieves SOTA performance."
    },
    {
        "title": "Attentive Tree-structured Network for Monotonicity Reasoning",
        "imageSrc": "images/att_tree_lstm.png",
        "imageSrcset": "images/att_tree_lstm-p-500.png 500w, images/att_tree_lstm-p-800.png 800w, images/att_tree_lstm-p-1080.png 1080w, images/att_tree_lstm-p-1600.png 1600w, images/att_tree_lstm.png 1848w",
        "pdfLink": "https://aclanthology.org/2021.naloma-1.3/",
        "workshopLink": "https://typo.uni-konstanz.de/naloma21/index.html",
        "workshopLabel": "NALOMA 2020",
        "authors": "Zeming Chen",
        "publicationInfo": "Proceedings of the 1st and 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA 2020, 2021)",
        "presentationType": null,
        "summary": "An attentive tree-structured neural network that consists of a tree-based long-short-term-memory network (Tree-LSTM) with soft attention designed to model the syntactic parse tree information from the sentence pair of a reasoning task."
    }

]
